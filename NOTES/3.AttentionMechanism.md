## Query, Key, and Value in Self-Attention

### Where Query, Key, and Value Come From
        Query (Q), Key (K), and Value (V) vectors are not predefined or manually assigned. They are learned during training, just like embeddings or any other neural network weights.

        After tokenization, embedding, and positional encoding, the input to a Transformer layer is a matrix X with shape:
            X ∈ R^(sequence length × d_model)
        Each row of X represents a token enriched with positional information.
        The Transformer initializes three learnable weight matrices:
            W_Q ∈ R^(d_model × d_k)****
            W_K ∈ R^(d_model × d_k)
            W_V ∈ R^(d_model × d_v)

        These matrices are randomly initialized at the start of training and are updated through backpropagation. Their purpose is to project the same input X into three different vector spaces.
    ---

    ### Generating Q, K, and V Vectors
        For every token in the sequence, Q, K, and V are computed using linear projections:
            Q = X · W_Q
            K = X · W_K
            V = X · W_V

        This computation happens in parallel for all tokens.

        Resulting shapes:
            Q ∈ R^(sequence length × d_k)
            K ∈ R^(sequence length × d_k)
            V ∈ R^(sequence length × d_v)

        Each token now has three vectors:
            * A query vector describing what the token is looking for
            * A key vector describing what the token represents
            * A value vector containing the information to pass forward
        These roles are not semantic by design. Their meaning emerges purely from training.
    ---

    ### Intuition Behind Query, Key, and Value
        The attention mechanism can be compared to information retrieval:
        * Queries ask questions
        * Keys describe available information
        * Values contain the actual information
        A token uses its query to compare against the keys of all tokens, including itself, to determine relevance.
    ---

    ### Step 1: Computing Attention Scores
        Each query vector is compared with all key vectors using a dot product:
            Scores = Q · Kᵀ
        The result is a square matrix of shape:
            (sequence length × sequence length)
        Entry (i, j) represents how strongly token i should attend to token j.
    ---

    ### Step 2: Scaling the Scores
        Dot products grow large when d_k is large, which can destabilize training. To fix this, the scores are scaled:
            Scaled Scores = Scores / √d_k
        This keeps gradients well behaved.
    ---

    ### Step 3: Applying Softmax
        Softmax is applied row-wise to convert scores into probabilities:
            Attention Weights = softmax(Scaled Scores)
        Each row sums to 1. This represents how attention is distributed across tokens for a given query.
    ---

    ### Step 4: Combining Values
        The attention weights are used to compute a weighted sum of value vectors:
            Output = Attention Weights · V
        The result has shape:
            (sequence length × d_v)

        Each output vector is a context-aware representation of the corresponding token.
    ---

    ### Concrete Example
        Sentence:
        "I am a robot"

        Assume sequence length = 4 and d_model = 4 for simplicity.

        After embeddings and positional encoding, assume X is:
            X =
                [
                x₀,
                x₁,
                x₂,
                x₃
                ]

        Each xᵢ ∈ R⁴.

        Linear projections produce:
            qᵢ = xᵢ · W_Q
            kᵢ = xᵢ · W_K
            vᵢ = xᵢ · W_V

        For the word "robot" at position 3, its query vector q₃ is compared with all keys:
            q₃ · k₀
            q₃ · k₁
            q₃ · k₂
            q₃ · k₃

        After scaling and softmax, these scores become attention weights that determine how much information "robot" gathers from each word in the sentence.
        The final vector for "robot" is:
            output₃ = Σ(attention_weight₃ⱼ × vⱼ)
    ---

    ### Learning Through Backpropagation
        If the model makes a wrong prediction, the loss produces gradients that flow backward through:
            Output → Attention Weights → Q, K, V → W_Q, W_K, W_V → Embeddings
        Only the relevant rows and projections are updated. Over time, the model learns how to form useful queries, keys, and values.
    ---

    ### Multi-Head Attention (Brief)
        Instead of a single QKV projection, the model splits d_model into multiple heads. Each head performs attention independently, learning different relationships. The outputs are concatenated and projected back into d_model.
    ---

    ### End-to-End Summary

        Text → Token IDs → Embeddings → Positional Encoding → X
            → Linear projections (Q, K, V)
            → Dot products and scaling
            → Softmax
            → Weighted sum of values
            → Contextualized representations
            → Next Transformer layer

    This mechanism allows each token to dynamically attend to all other tokens based on learned relevance.

## Why Shared Q, K, V Weights Still Produce Correct Attention
    ### Core Idea
            In a Transformer, the Query, Key, and Value projection matrices are learned once per layer and shared across all tokens in the input sequence. Despite this sharing, each word still receives a unique and correct representation because the inputs to these projections are different for every token.
    ---

    ### What Is Shared and What Is Not
    The Transformer defines three learnable weight matrices:
        W_Q, W_K, and W_V

    These matrices are:
        * Randomly initialized at the start of training
        * Learned through backpropagation
        * Shared across all tokens and all positions in the sequence

    What is not shared are the input vectors. Each token has its own embedding combined with positional encoding.
    ---

    ### Input to the Attention Layer
        After tokenization, embedding, and positional encoding, the input to self-attention is a matrix X:
            X ∈ R^(sequence length × d_model)
        Each row x_i represents a different word at a different position. Even if two tokens are the same word, their positional encodings make their vectors different.
    ---

    ### How Q, K, and V Are Generated
        The same projection matrices are applied to every token:
            q_i = x_i · W_Q
            k_i = x_i · W_K
            v_i = x_i · W_V

        Because each x_i is different, the resulting q_i, k_i, and v_i vectors are also different, even though the transformation is the same.
        This is identical to applying the same mathematical function to different inputs and obtaining different outputs.
    ---

    ### Why Weight Sharing Is Essential
    If each word had its own Q, K, and V matrices:
    * The number of parameters would explode
    * The model would overfit
    * The model would fail to generalize to new sentences

    Sharing forces the model to learn reusable patterns rather than memorizing specific positions or words.
    ---

    ### How Correct Attention Emerges

    The embedding vectors already encode semantic meaning and positional information. The projection matrices learn how to interpret these embeddings for attention.

    During training:
    * Tokens in similar contexts receive similar gradient updates
    * Projection matrices learn how to extract relevant features
    * Attention patterns emerge naturally from optimization pressure

    The model learns rules such as which words should attend to others, without explicit supervision.
    ---

    ### Intuition That Holds
        The projection matrices act like shared lenses. Each word looks different through the same lens because each word’s input vector is different.
        Same transformation, different input, different result.
    ---

    ### One-Line Summary
        Q, K, and V matrices are shared to learn general attention rules, while unique token embeddings ensure each word receives its own context-aware representation.