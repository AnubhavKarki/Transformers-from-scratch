Dot Product's Significance:

## Aligned Vectors in Attention
    Aligned vectors are vectors that point in the same or nearly the same direction. Mathematically, this means the angle between them is small, and 
    their dot product is large and positive.

    In the context of Transformers and self-attention, alignment is crucial because it determines relevance between tokens. Specifically, the 
    attention score is computed as the dot product between a query vector (Q) and a key vector (K). When these vectors are aligned, the dot product 
    is high, indicating that the model should pay more attention to that word.

    Intuitive breakdown:
        * **Same direction** → vectors are highly aligned → high similarity → strong attention
        * **Perpendicular** → vectors have little to no alignment → near-zero similarity → weak or no attention
        * **Opposite direction** → vectors are anti-aligned → negative similarity → low or negative attention contribution

    Understanding vector alignment helps explain why the dot product is used in attention mechanisms: it effectively measures **how well the 
    query matches the key** in direction and magnitude, guiding the model to focus on relevant tokens while ignoring irrelevant ones.

Significance of Dot Products in Attention:
    The dot product is a fundamental operation in self-attention because it quantifies the alignment or similarity between two vectors. In Transformers, 
    attention scores are calculated as the dot product between a query vector (Q) and a key vector (K). The higher the dot product, the more aligned the 
    vectors are, which indicates that the corresponding tokens are highly relevant to each other.

    Key points:
        Magnitude matters: A larger dot product not only reflects alignment in direction but also takes the vector magnitudes into account, allowing 
                            stronger signals for important tokens.

        Geometric intuition: The dot product is proportional to the cosine of the angle between vectors multiplied by their magnitudes. Small angles 
                                (high alignment) produce high positive scores.

        * Attention scoring: By computing Q · Kᵀ for all token pairs, the model generates a matrix of relevance scores. These scores guide how much 
                                information each token should receive from every other token in the sequence.
                                
        * Efficiency: Using dot products allows parallel computation for all token pairs, making attention scalable and computationally efficient.

    In short, dot products are critical because they provide a simple, mathematically efficient way to measure similarity, which directly drives the distribution of 
    attention across tokens.