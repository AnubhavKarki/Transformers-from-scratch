**Multi-Head Attention (MHA)**

**Definition:**
Multi-Head Attention is a key component of the Transformer architecture. It allows the model to focus on different parts of the input sequence simultaneously, capturing diverse contextual relationships.

**Core Idea:**

* Instead of computing a single attention function, MHA runs multiple attention layers (called "heads") in parallel.
* Each head learns different representations or features from the input.

**Formulation:**

1. **Input:** Sequence of vectors (X \in \mathbb{R}^{n \times d_{model}})

2. **Linear Projections:** For each head, compute:

   * Queries (Q = X W_Q)
   * Keys (K = X W_K)
   * Values (V = X W_V)
     where (W_Q, W_K, W_V \in \mathbb{R}^{d_{model} \times d_k})

3. **Scaled Dot-Product Attention:**
   [
   Attention(Q,K,V) = softmax\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
   ]

* Scale factor (\sqrt{d_k}) prevents large dot-product values which could push softmax into regions with small gradients.

4. **Concatenate Heads:**

* Each head output (head_i = Attention(Q_i, K_i, V_i))
* Concatenate all heads: (Concat(head_1,...,head_h))

5. **Final Linear Layer:**

* Apply a linear transformation: (MultiHead(Q,K,V) = Concat(heads) W_O), where (W_O \in \mathbb{R}^{h*d_k \times d_{model}})

**Advantages:**

* Captures information from different representation subspaces.
* Enables the model to focus on multiple positions in the sequence.
* Improves learning of complex relationships compared to single-head attention.

**Notes:**

* Typical settings: (d_{model} = 512), (h = 8) heads, (d_k = d_v = d_{model}/h = 64)
* Widely used in NLP (BERT, GPT) and computer vision (Vision Transformers).

**Visual Intuition:**

* Each head is like an independent lens looking at the sequence from a different perspective.
* By combining all heads, the model gains a holistic understanding of the input.