## Softmax Function in Attention
After computing the raw attention scores via dot products between query and key vectors, the **softmax function** is applied. 
Softmax converts these raw scores into a probability distribution across all tokens, ensuring that all attention weights for a given token sum to 1. 
This allows the model to interpret the scores as **relative importance**, determining how much each token should contribute to the final representation.

Key points:
* **Probability interpretation**: Each token’s attention weight represents the proportion of focus it receives from the current token.
* **Focus mechanism**: Softmax emphasizes the most relevant tokens (larger scores) while suppressing less relevant ones (smaller scores).
* **Numerical stability**: Softmax ensures the output values are bounded and suitable for downstream multiplication with value vectors.

## Scaling (Normalization) in Attention
Before applying softmax, raw attention scores are often **scaled** by dividing by the square root of the key dimension (√d_k).
This is crucial because dot products grow with the dimensionality of the vectors, which can lead to extremely large values. 
Large values can push softmax into regions with very small gradients, making training unstable.

Key points:
* **Prevents extreme values**: Scaling keeps dot product values in a range where softmax is sensitive and produces meaningful gradients.
* **Stable training**: Avoids vanishing or exploding gradients in the attention mechanism.
* **Formula**: Scaled Score = (Q · Kᵀ) / √d_k

In combination, scaling and softmax ensure that attention weights are meaningful, stable, and interpretable as probabilities that guide 
how each token attends to others.